name: scrape

on:
  schedule:
    - cron: '*/5 * * * *'  # every 5 minutes
  workflow_dispatch:       # allows you to trigger it manually from GitHub

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.x

      - name: Install dependencies
        run: pip install -r requirements.txt

      # ⬇️ STEP: Download saved data.json
      - name: Download saved posts
        uses: actions/download-artifact@v3
        with:
          name: saved-posts
          path: .

      - name: Run scraper
        run: python scraper.py

      # ⬆️ STEP: Upload updated data.json
      - name: Upload saved posts
        uses: actions/upload-artifact@v3
        with:
          name: saved-posts
          path: data.json
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run scraper
        env:
          PUSHBULLET_API_KEY: ${{ secrets.PUSHBULLET_API_KEY }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
          EMAIL_TO: ${{ secrets.EMAIL_TO }}
        run: python scraper.py

      - name: Upload saved posts
        uses: actions/upload-artifact@v3
        with:
          name: saved-posts
          path: data.json
